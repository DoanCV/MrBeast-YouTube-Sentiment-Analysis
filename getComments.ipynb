{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "getComments.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMrUEolayg10U4xK6n8oO7v"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtK9V3q3me0_"
      },
      "source": [
        "# Import libraries\n",
        "import json\n",
        "from csv import writer\n",
        "from apiclient.discovery import build\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import urllib.request\n",
        "import urllib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeIxZhFZq9J5"
      },
      "source": [
        "# API key from YouTube Data API key\n",
        "key = <'Key'>\n",
        "videoId = 'PKtnafFtfEo' # Youtube Rewind 2020, Thank God It's Over\n",
        "channelId = 'UCX6OQ3DkcsbYNE6H8uQQuVA' # MrBeast "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--zxJNNUq_p_"
      },
      "source": [
        "def build_service():\n",
        "    YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
        "    YOUTUBE_API_VERSION = \"v3\"\n",
        "    return build(YOUTUBE_API_SERVICE_NAME,\n",
        "                 YOUTUBE_API_VERSION,\n",
        "                 developerKey=key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o87R1k2S75oF"
      },
      "source": [
        "def get_comments(part='snippet', \n",
        "                 maxResults=100, \n",
        "                 textFormat='plainText',\n",
        "                 order='time',\n",
        "                 allThreadsRelatedToChannelId=channelId,\n",
        "                 csv_filename=\"MrBeast_comments\"\n",
        "                 ):\n",
        "  \n",
        "  # Initialize empty lists which will store the stats on a video\n",
        "  comments, commentsId, authorurls, authornames, repliesCount, likesCount, viewerRating, dates, vidIds, totalReplyCounts,vidTitles = [], [], [], [], [], [], [], [], [], [], []\n",
        "\n",
        "  # Build the serivce\n",
        "  service = build_service()\n",
        "\n",
        "  # Call the API with the service\n",
        "  response = service.commentThreads().list(\n",
        "        part=part,\n",
        "        maxResults=maxResults,\n",
        "        textFormat='plainText',\n",
        "        order=order,\n",
        "        allThreadsRelatedToChannelId=channelId\n",
        "  ).execute()\n",
        "\n",
        "  # There is a limit to the amount of data I can pull. \n",
        "  # For MrBeast, his channel is quite large and active so this loop will stop running at the quota\n",
        "  # Typically I would set a hard cap on the number of comments to pull that is lower than the quota to test if the function is working properly but I have a guide on this API sourced below.\n",
        "\n",
        "  while response:\n",
        "    for item in response['items']:\n",
        "            # Index item for desired data features\n",
        "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "            comment_id = item['snippet']['topLevelComment']['id']\n",
        "            reply_count = item['snippet']['totalReplyCount']\n",
        "            like_count = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
        "            authorurl = item['snippet']['topLevelComment']['snippet']['authorChannelUrl']\n",
        "            authorname = item['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
        "            date = item['snippet']['topLevelComment']['snippet']['publishedAt']\n",
        "            vidId = item['snippet']['topLevelComment']['snippet']['videoId']\n",
        "            totalReplyCount = item['snippet']['totalReplyCount']\n",
        "            vidTitle = get_vid_title(vidId)\n",
        "\n",
        "            # Append to lists\n",
        "            comments.append(comment)\n",
        "            commentsId.append(comment_id)\n",
        "            repliesCount.append(reply_count)\n",
        "            likesCount.append(like_count)\n",
        "            authorurls.append(authorurl)\n",
        "            authornames.append(authorname)\n",
        "            dates.append(date)\n",
        "            vidIds.append(vidId)\n",
        "            totalReplyCounts.append(totalReplyCount)\n",
        "            vidTitles.append(vidTitle)\n",
        "\n",
        "    try:\n",
        "            if 'nextPageToken' in response:\n",
        "                response = service.commentThreads().list(\n",
        "                    part=part,\n",
        "                    maxResults=maxResults,\n",
        "                    textFormat=textFormat,\n",
        "                    order=order,\n",
        "                    allThreadsRelatedToChannelId=channelId,\n",
        "                    pageToken=response['nextPageToken']\n",
        "                ).execute()\n",
        "            else:\n",
        "                break\n",
        "    except: break\n",
        "  \n",
        "  return {\n",
        "        'comment': comments,\n",
        "        'comment_id': commentsId,\n",
        "        'author_url': authorurls,\n",
        "        'author_name': authornames,\n",
        "        'reply_count' : repliesCount,\n",
        "        'like_count' : likesCount,\n",
        "        'date': dates,\n",
        "        'vidid': vidIds,\n",
        "        'total_reply_counts': totalReplyCounts,\n",
        "        'vid_title': vidTitles\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTVDommn75qx"
      },
      "source": [
        "def get_vid_title(vidid):\n",
        "    # VideoID = \"LAUa5RDUvO4\"\n",
        "    # Usually, the videoID is after the ?v=. This information can also be accessed when clicking share on the video itself.\n",
        "    params = {\"format\": \"json\", \"url\": \"https://www.youtube.com/watch?v=%s\" % vidid}\n",
        "    url = \"https://www.youtube.com/oembed\"\n",
        "    query_string = urllib.parse.urlencode(params)\n",
        "    url = url + \"?\" + query_string\n",
        "\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        response_text = response.read()\n",
        "        data = json.loads(response_text.decode())\n",
        "        return data['title']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqOEvueArFNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "130850a7-b7f0-4455-9028-18ebd5c6bf4e"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    MrBeast_comments = get_comments()\n",
        "    df = pd.DataFrame(MrBeast_comments)\n",
        "    print(df.shape)\n",
        "    print(df.head())\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    df['just_date'] = df['date'].dt.date\n",
        "    df.to_csv('./MrBeast_comments.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11258, 10)\n",
            "                                             comment  ...                                          vid_title\n",
            "0  \"I'm sorry, international fans\"\\n\\n*Cries in P...  ...   I Opened A Restaurant That Pays You To Eat At It\n",
            "1                                          Relatable  ...  I Gave People $1,000,000 But ONLY 1 Minute To ...\n",
            "2                      How do you Gus love that crap  ...                 I Went Back To 1st Grade For A Day\n",
            "3  I wouldnâ€™t be surprised if MrBeast became one ...  ...                    I Bought Everything In 5 Stores\n",
            "4                               Chandler:eats a rose  ...  Surprising My Girlfriend With 100,000 Roses Fo...\n",
            "\n",
            "[5 rows x 10 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}